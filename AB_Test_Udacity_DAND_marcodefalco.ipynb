{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing A/B Test Results of an e-commerce website\n",
    "\n",
    "This project was part of the Data Analyst NanoDegree program offere on Udacity. The idea is to apply the statistical skills I acquired and showcase them here, in a practical project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.\n",
    "\n",
    "For this project, I will be working to understand the results of an A/B test run by an e-commerce website.  My goal is to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "<a id='probability'></a>\n",
    "### Part I - Probability\n",
    "\n",
    "I will use this part to showcase my understanding of probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#I am setting the seed to assure I comply with the same answers of Udacity\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset import and minimal datset exploration\n",
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294478, 5)\n",
      "290584\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df['user_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I calculate the proportion of the users converted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12104245244060237"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note that I use nunique(), as I need to count the DISTINCT users that converted\n",
    "df.query('converted == \"1\"').user_id.nunique() / df['user_id'].nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times the new_page and treatment don't line up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_mismatch = df.query('group == \"control\" and landing_page ==\"new_page\"' )\n",
    "b_mismatch = df.query('group == \"treatment\" and landing_page ==\"old_page\"' )\n",
    "\n",
    "a_mismatch.shape[0] + b_mismatch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, timestamp, group, landing_page, converted]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do we have any missing values? Apparently no.\n",
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rows where treatment is not aligned with new_page or control is not aligned with old_page, we cannot be sure if this row truly received the new or old page. Use Quiz 2 in the classroom to provide how we should handle these rows.\n",
    "\n",
    "a. Now I have to create two different df because of the mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(a_mismatch.index)\n",
    "df2 = df1.drop(b_mismatch.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.duplicated(['user_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop_duplicates(subset='user_id')\n",
    "df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.query('converted == \"1\"').user_id.nunique() / df2['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that an individual was in the control group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_conv_ctrl = df2.query('converted == \"1\" and group == \"control\"').user_id.nunique() / df2.query('group == \"control\"').user_id.nunique()\n",
    "p_conv_ctrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that an individual was in the treatment group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_conv_treat = df2.query('converted == \"1\" and group == \"treatment\"').user_id.nunique() / df2.query('group == \"treatment\"').user_id.nunique()\n",
    "p_conv_treat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_conv_treat - p_conv_ctrl # !! Keep in mind that later, under the null, we are assuming this difference to be 0 anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.query('landing_page == \"new_page\"').user_id.nunique() / df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can we conclude at this point?\n",
    "Not much really. The control group obtained slightly higher conversions than treatment group. However, not only the difference is minimal, but also **we did not run any statistical test in order to substantiate our findings (no p- value, no z-tests calculated).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "f I want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should our null and alternative hypotheses be?  We can state your hypothesis in terms of words or in terms of **$p_{old}$** and **$p_{new}$**, which are the converted rates for the old and new pages.\n",
    "\n",
    "**$H_{null}: p_{old} - p_{new} \\leq 0$**   <br>\n",
    "**$H_{alt}: p_{new} - p_{old} > 0$**\n",
    "\n",
    "The Hypothesis that we want to reject is that, on average, the conversion rate of the old page is better or equal to the new page. This is our starting point and what we are trying to disprove. \n",
    "Conversely, the alternative Hypothesis is that, on average, the conversion rate of the new page is better than the old page. \n",
    "It is also important to keep in mind the standard 0.05 threshold for the statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now I will ask a few questions to show the typical way of reasoning in this situations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the convert rate for  pnew  under the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As the null Hp states that on average the conversion rate of Pold will be greater or equal to the Pnew,\n",
    "# what is the highest possible value that Pnew can have under the null? Pnew = Pold . \n",
    "# So I set p_new equal to the mean \n",
    "p_new = df2.converted.mean()\n",
    "p_new\n",
    "# I have to assume not only that Pnew and Pold are equal, \n",
    "# but also that they are equal to the converted rate in ab_data.csv regardless of the page.\n",
    "# I should make sense now given the assumptions stated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the convert rate for  pold  under the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ..consequently:\n",
    "p_old = df2.converted.mean()\n",
    "p_old\n",
    "# Pold = Pnew is basically the farthermost level at which the Null Hp stands true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the sample of Pnew (n_new) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If I calculated things correctly, there should be only unique user_ids per each group/landing_page,\n",
    "# So I can go this way..\n",
    "n_new = df2.query('group == \"treatment\"').shape[0]\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is  n_old ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_old = df2.query('group == \"control\"').shape[0]\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate  n_new  transactions with a convert rate of  p_new  under the null. Store these  n_new  1's and 0's in new_page_converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I simulate n_new transactions\n",
    "# I am recalculating the sample basically. I compute the p_new probability to get converted (1 means conversion\n",
    "# 0 means no conversion), n_new times.\n",
    "new_page_converted = []\n",
    "for _ in range(n_new):\n",
    "    a = np.random.binomial(1, p_new)\n",
    "    new_page_converted.append(a)\n",
    "    \n",
    "new_page_converted = np.asarray(new_page_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate  nold  transactions with a convert rate of  pold  under the null. Store these  nold  1's and 0's in old_page_converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for n_old\n",
    "old_page_converted = []\n",
    "for _ in range(n_old):\n",
    "    b = np.random.binomial(1, p_old)\n",
    "    old_page_converted.append(b)\n",
    "    \n",
    "old_page_converted = np.asarray(old_page_converted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find  pnew  -  pold  for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017048227728868326"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_pnew_minus_pold = (new_page_converted.mean() - old_page_converted.mean())\n",
    "sim_pnew_minus_pold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Simulate 10,000  pnew  -  pold  values using this same process similarly to the one you calculated in parts a. through g. above. Store all 10,000 values in p_diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is bootstrap sampling\n",
    "# I could nest for loops but this is probably faster.\n",
    "p_diffs = []\n",
    "for _ in range(10000):\n",
    "    bs_new_page_c = np.random.binomial(1, p_new, n_new)\n",
    "    bs_old_page_c = np.random.binomial(1, p_old, n_old)\n",
    "    p_diffs.append(bs_new_page_c.mean()-bs_old_page_c.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a histogram of the p_diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCQ1ygCQpEYZAJLUqP8HER/3WbBWwFQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1099c4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_diff = p_conv_treat - p_conv_ctrl\n",
    "plt.hist(p_diffs)\n",
    "plt.axvline(actual_diff, c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the p_diffs are greater than the actual difference observed in ab_data.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90429999999999999"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the p-value here.\n",
    "# I have to turn p_diffs to an array, if I do the following with p_diffs as a list, it won't work.\n",
    "p_diffs = np.asarray(p_diffs)\n",
    "(p_diffs > actual_diff).mean() #In how many cases the calculated differences (from the simulations) were higher than the actual differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Slightly over 90% of the proportional differences we calculated in our simulation was greater than the actual difference (marked by the red line). In other words, we wanted to compute, under our null hypothesis, the probability of seeing a difference in conversions as extreme or more extreme than the actual difference. This is actually the p-value of our study, which states that in 90% of the cases we would see results as extreme or more extreme than the actual differences. When we started, we had set a p-value of 0.05, which implies that we have to consider this result, not statistically significant.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use a built-in to achieve similar results. Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance. Fill in the below to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let n_old and n_new refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = df2.query('group == \"control\" and converted == 1').user_id.nunique()  #  we had this from part I Q4.\n",
    "convert_new = df2.query('group == \"treatment\" and converted == 1').user_id.nunique() # this one too.\n",
    "n_old = df2.query('group == \"control\"').user_id.nunique()\n",
    "n_new = df2.query('group == \"treatment\"').user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use stats.proportions_ztest to compute the test statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3109241984234394, 0.18988337448195103)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new])\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have to watch out! why does the p-value of the built in disagrees with what we have calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.094941687240975514"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The p-value above has not been calculated correctly given the specific conditions of our research!!\n",
    "# Above, the p-value was calculated as both tails of the distribution were to be considered, but we should\n",
    "# remember that the Null Hp was not Pnew = Pold, but Pold >= Pnew. This means we need only one tail.\n",
    "one_tail = p_value/2\n",
    "one_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90505831275902449"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- one_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should recall that our investigation is \"one tailed\". In other words, we have a null Hp that states \"p_new - p_old <= 0, rather than \"p_new - p_old = 0\". The built in function is more compatible with a two-tailed Hypothesis test. In such test the null Hp could be stated in this fashion: p_new - pold = 0 and the alternative Hp being $p{new} - p_{old} \\neq 0$ . Therefore we should half the calculated p-value to get the result we want. I also included a subtraction of the resulting value from 1, to see how close it gets to the value we originally calculated in question j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, I will show that the result acheived in the previous A/B test can also be acheived by performing regression.<br><br>\n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should I be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use statsmodels to fit the regression model you specified in part a. to see if there is a significant difference in conversion based on which page a customer receives. However, you first need to create a colun for the intercept, and create a dummy variable column for which page each user received. Add an intercept column, as well as an ab_page column, which is 1 when an individual receives the treatment and 0 if control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>old_page</th>\n",
       "      <th>new_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  old_page  new_page  \n",
       "0          1         0         1  \n",
       "1          1         0         1  \n",
       "2          1         1         0  \n",
       "3          1         1         0  \n",
       "4          1         0         1  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['intercept'] = 1\n",
    "df2[['old_page', 'new_page']] = pd.get_dummies(df2['landing_page'])\n",
    "df2.head() # I needed two passages to create the ab_page, although I see it's just what I labeled old_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['ab_page'] = df2['old_page']\n",
    "df2 = df2.drop(['old_page', 'new_page'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use statsmodels to import your regression model. Instantiate the model, and fit the model using the two columns you created in part b. to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# I have inserted the lamba func as the code was not working\n",
    "# I found the solution here https://github.com/statsmodels/statsmodels/issues/3931\n",
    "# I also asked the mentor, it's because of chisqprob deprecation.\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "logit_mod = sm.Logit(df2['converted'],df2[['intercept','ab_page']])\n",
    "results = logit_mod.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Thu, 26 Apr 2018</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:00:45</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Thu, 26 Apr 2018   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        16:00:45   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with ab_page? Why does it differ from the value I found in the Part II?\n",
    "\n",
    "Hint: What are the null and alternative hypotheses associated with your regression model, and how do they compare to the null and alternative hypotheses in the Part II?\n",
    "\n",
    "The p-value associated with ab_page is 0.190, the double of the one we calculated in part II. \n",
    "\n",
    "The null and alternative hypothesis in the regression model are as follows: <br>\n",
    "HO :  pnew  -  pold=0  \n",
    "\n",
    "HA :  pnew  -  pold≠0  \n",
    "\n",
    "**The difference with our alternative and null Hypothesis stated in Part II is clear: The analysis of the regression model implies a two-tailed test. Our prior investigation was one-tailed.**\n",
    "\n",
    "f. Now, why it is a good idea to consider other factors to add into your regression model? Are there any disadvantages to adding additional terms into your regression model?\n",
    "\n",
    "The first disadvantgage of adding new features is the increased probability in collinearity. New features can also impact the significance of the analysis, deflating our p-value. However, adding new features can show new correlations, which in turn can give us more insight on what can help us improve our conversions, the point, once again, is how the statistical tests will behave after the inclusion of the new features.\n",
    "\n",
    "g. Now along with testing if the conversion rate changes for different pages, I also add an effect based on which country a user lives. I will need to read in the countries.csv dataset and merge together your datasets on the approporiate rows.\n",
    "\n",
    "Does it appear that country had an impact on conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv('./countries.csv')\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.country.unique() # As the description states: 3 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>653118</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-09 03:12:31.034796</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878226</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-05 15:02:50.334962</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799368</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-09 18:07:34.253935</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655535</th>\n",
       "      <td>CA</td>\n",
       "      <td>2017-01-09 13:30:47.524512</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934996</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-09 00:30:08.377677</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "653118       US  2017-01-09 03:12:31.034796    control     old_page   \n",
       "878226       UK  2017-01-05 15:02:50.334962    control     old_page   \n",
       "799368       UK  2017-01-09 18:07:34.253935    control     old_page   \n",
       "655535       CA  2017-01-09 13:30:47.524512  treatment     new_page   \n",
       "934996       UK  2017-01-09 00:30:08.377677    control     old_page   \n",
       "\n",
       "         converted  intercept  ab_page  CA  UK  US  \n",
       "user_id                                             \n",
       "653118           0          1        0   0   0   1  \n",
       "878226           0          1        0   0   1   0  \n",
       "799368           0          1        0   0   1   0  \n",
       "655535           0          1        1   1   0   0  \n",
       "934996           0          1        0   0   1   0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create the necessary dummy variables, assigning the proper labels.\n",
    "df_new[['CA', 'UK', 'US']] = pd.get_dummies(df_new['country'])\n",
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Thu, 26 Apr 2018</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:06:08</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9794</td> <td>    0.013</td> <td> -155.415</td> <td> 0.000</td> <td>   -2.004</td> <td>   -1.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>   -0.0099</td> <td>    0.013</td> <td>   -0.743</td> <td> 0.457</td> <td>   -0.036</td> <td>    0.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0506</td> <td>    0.028</td> <td>   -1.784</td> <td> 0.074</td> <td>   -0.106</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Thu, 26 Apr 2018   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        16:06:08   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9794      0.013   -155.415      0.000      -2.004      -1.954\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "US            -0.0099      0.013     -0.743      0.457      -0.036       0.016\n",
       "CA            -0.0506      0.028     -1.784      0.074      -0.106       0.005\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping UK so it can be our baseline\n",
    "logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','CA']])\n",
    "results = logit_mod.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.99014884368295719, 0.95065885803307082)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.0099), np.exp(-0.0506) \n",
    "#Here is in \"unit increase\", though the value is below 1... However, I find it more intuitive this way, without calculating the reciprocal\n",
    "#If an individual lives in the US, it is 0.99 times as likely to convert than the baseline (UK).\n",
    "#If an individual lives in Canada, it is 0.95 times as likely to convert than the baseline (UK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.138154\n",
       "ab_page      0.985168\n",
       "US           0.990165\n",
       "CA           0.950621\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(results.params) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we should restate one more time that this model implies a two tailed Hp test: \n",
    "\n",
    "pnew−pold=0  and the alternative Hp being:  pnew−pold≠0  \n",
    "\n",
    "Under these conditions, the Logit regressions results still tell us that there is no significance at 0.05 threshold.\n",
    "\n",
    "Yet, if we were to assume these results as significant, we would conclude the following: \n",
    "If an individual lives in the US, it is 0.99 times as likely to convert than the baseline (UK).\n",
    "If an individual lives in Canada, it is 0.95 times as likely to convert than the baseline (UK).\n",
    "If an individual sees the new page, it is 0.985 times as likely to convert holding everything else equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though I have now looked at the individual factors of country and page on conversion, I would now like to look at an interaction between page and country to see if there significant effects on conversion. I'll create the necessary additional columns, and fit the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>US_ab</th>\n",
       "      <th>CA_ab</th>\n",
       "      <th>UK_ab</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>834778</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928468</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822059</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711597</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710616</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909908</th>\n",
       "      <td>UK</td>\n",
       "      <td>2017-01-06 20:44:26.334764</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811617</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-02 18:42:11.851370</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938122</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-10 09:32:08.222716</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887018</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-06 11:09:40.487196</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820683</th>\n",
       "      <td>US</td>\n",
       "      <td>2017-01-14 11:52:06.521342</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        country                   timestamp      group landing_page  \\\n",
       "user_id                                                               \n",
       "834778       UK  2017-01-14 23:08:43.304998    control     old_page   \n",
       "928468       US  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "822059       UK  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "711597       UK  2017-01-22 03:14:24.763511    control     old_page   \n",
       "710616       UK  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "909908       UK  2017-01-06 20:44:26.334764  treatment     new_page   \n",
       "811617       US  2017-01-02 18:42:11.851370  treatment     new_page   \n",
       "938122       US  2017-01-10 09:32:08.222716  treatment     new_page   \n",
       "887018       US  2017-01-06 11:09:40.487196  treatment     new_page   \n",
       "820683       US  2017-01-14 11:52:06.521342  treatment     new_page   \n",
       "\n",
       "         converted  intercept  ab_page  CA  UK  US  US_ab  CA_ab  UK_ab  \n",
       "user_id                                                                  \n",
       "834778           0          1        0   0   1   0      0      0      0  \n",
       "928468           0          1        1   0   0   1      1      0      0  \n",
       "822059           1          1        1   0   1   0      0      0      1  \n",
       "711597           0          1        0   0   1   0      0      0      0  \n",
       "710616           0          1        1   0   1   0      0      0      1  \n",
       "909908           0          1        1   0   1   0      0      0      1  \n",
       "811617           1          1        1   0   0   1      1      0      0  \n",
       "938122           1          1        1   0   0   1      1      0      0  \n",
       "887018           0          1        1   0   0   1      1      0      0  \n",
       "820683           0          1        1   0   0   1      1      0      0  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are creating new columns to further categorize the data.\n",
    "# e.g. a user from Canada that views the old page will have a CA_ab value of 0, a user from US, new page, US_ab 1\n",
    "# This granular view will allow us to see if there are interaction between specific countries and the 2 page variants\n",
    "df_new['US_ab'] = np.multiply(df_new['ab_page'],df_new['US'])\n",
    "df_new['CA_ab'] = np.multiply(df_new['ab_page'],df_new['CA'])\n",
    "df_new['UK_ab'] = np.multiply(df_new['ab_page'],df_new['UK'])\n",
    "\n",
    "df_new.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Thu, 26 Apr 2018</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:09:38</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9922</td> <td>    0.016</td> <td> -123.457</td> <td> 0.000</td> <td>   -2.024</td> <td>   -1.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>    0.0108</td> <td>    0.023</td> <td>    0.475</td> <td> 0.635</td> <td>   -0.034</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>    0.0057</td> <td>    0.019</td> <td>    0.306</td> <td> 0.760</td> <td>   -0.031</td> <td>    0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0118</td> <td>    0.040</td> <td>   -0.296</td> <td> 0.767</td> <td>   -0.090</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US_ab</th>     <td>   -0.0314</td> <td>    0.027</td> <td>   -1.181</td> <td> 0.238</td> <td>   -0.084</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA_ab</th>     <td>   -0.0783</td> <td>    0.057</td> <td>   -1.378</td> <td> 0.168</td> <td>   -0.190</td> <td>    0.033</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Thu, 26 Apr 2018   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        16:09:38   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "                                        LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9922      0.016   -123.457      0.000      -2.024      -1.961\n",
       "ab_page        0.0108      0.023      0.475      0.635      -0.034       0.056\n",
       "US             0.0057      0.019      0.306      0.760      -0.031       0.043\n",
       "CA            -0.0118      0.040     -0.296      0.767      -0.090       0.066\n",
       "US_ab         -0.0314      0.027     -1.181      0.238      -0.084       0.021\n",
       "CA_ab         -0.0783      0.057     -1.378      0.168      -0.190       0.033\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_mod = sm.Logit(df_new['converted'],df_new[['intercept','ab_page','US','CA','US_ab','CA_ab']])\n",
    "results = logit_mod.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0814470441230692, 1.0318981806179213, 1.011869894648401, 0.9943162141784333)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/np.exp(-0.0783), 1/np.exp(-0.0314), 1/np.exp(-0.0118), 1/np.exp(0.0057)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    0.136392\n",
       "ab_page      1.010893\n",
       "US           1.005761\n",
       "CA           0.988285\n",
       "US_ab        0.969090\n",
       "CA_ab        0.924703\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(results.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no significant effects on conversions. Yet, if we were to assume these results as significant, we would conclude the following: \n",
    "If an individual lives in the US and sees the new page, it is 0.97 times as likely to convert than the baseline (UK).\n",
    "If an individual lives in Canada and sees the new page, it is 0.92 times as likely to convert than the baseline (UK)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "After going through the analysis, there are a few points we should highlight. Firstly, the practical significance of the change is minimal. In other words, even if the test results were highly significant, changing to a new page would not have that big of an impact on conversions. Moreover, as we saw, there is no sufficient support to our alternative hypothesis in the first place. This does not necessarily mean the new page is bad. In fact, we do not know for how much time the A/B test has been running, and when the page change was implemented. As we studied in the lectures, an increased sample size (more time spent A/B-testing) will tend to increase the significance of our study. Moreover, from the practical point of view, it might well be that the users need time to adjust to the new page (novelty effect), which in the long run could perform better than the old one.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
